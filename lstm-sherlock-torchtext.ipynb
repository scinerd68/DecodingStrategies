{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Train model","metadata":{"id":"4ckXupHfnIxM"}},{"cell_type":"code","source":"!pip show nltk","metadata":{"execution":{"iopub.status.busy":"2022-07-21T13:44:20.201872Z","iopub.execute_input":"2022-07-21T13:44:20.202662Z","iopub.status.idle":"2022-07-21T13:44:33.138143Z","shell.execute_reply.started":"2022-07-21T13:44:20.202535Z","shell.execute_reply":"2022-07-21T13:44:33.136740Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import re\nimport nltk\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nnltk.download('punkt')","metadata":{"id":"NtEItlej-5Hg","executionInfo":{"status":"ok","timestamp":1654511218754,"user_tz":-420,"elapsed":2455,"user":{"displayName":"Thang Phan","userId":"13927789615765580187"}},"outputId":"8c402d40-6904-46c6-c994-492903620fc5","execution":{"iopub.status.busy":"2022-07-21T13:44:33.140211Z","iopub.execute_input":"2022-07-21T13:44:33.140621Z","iopub.status.idle":"2022-07-21T13:44:35.294096Z","shell.execute_reply.started":"2022-07-21T13:44:33.140582Z","shell.execute_reply":"2022-07-21T13:44:35.292308Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from torchtext.vocab import FastText\nembedding = FastText('simple')","metadata":{"execution":{"iopub.status.busy":"2022-07-21T13:44:35.296020Z","iopub.execute_input":"2022-07-21T13:44:35.296654Z","iopub.status.idle":"2022-07-21T13:45:00.841061Z","shell.execute_reply.started":"2022-07-21T13:44:35.296595Z","shell.execute_reply":"2022-07-21T13:45:00.839925Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-07-21T13:45:00.843408Z","iopub.execute_input":"2022-07-21T13:45:00.844116Z","iopub.status.idle":"2022-07-21T13:45:00.848827Z","shell.execute_reply.started":"2022-07-21T13:45:00.844079Z","shell.execute_reply":"2022-07-21T13:45:00.847679Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import torch\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2022-07-21T13:45:00.850572Z","iopub.execute_input":"2022-07-21T13:45:00.851037Z","iopub.status.idle":"2022-07-21T13:45:00.893566Z","shell.execute_reply.started":"2022-07-21T13:45:00.850988Z","shell.execute_reply":"2022-07-21T13:45:00.892550Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\nimport re\n\nclass SherlockDataset(torch.utils.data.Dataset):\n    def __init__(self, text_path, word_embedding, sequence_length):\n        self.word_embedding = word_embedding\n        self.sequence_length = sequence_length\n        self.words = self.load_words(text_path)\n        self.uniq_words = self.get_uniq_words()\n\n        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n\n        self.words_indexes = [self.word_to_index[w] for w in self.words]\n\n    def load_words(self, text_path):\n        word_list = []\n        with open(text_path, 'r') as f:\n            text = f.read()\n        # Split text into sentences\n        sentences = sent_tokenize(text.strip())\n\n        for sent in sentences:\n            tokenized_sent = word_tokenize(sent.lower())\n            # Add end of sentence token\n            tokenized_sent.append('</s>')\n            # Remove punctuation\n            remove_punct = [word for word in tokenized_sent if re.search(r'\\w+', word) is not None]\n            word_list.extend(remove_punct)\n\n        return word_list\n    \n    def get_uniq_words(self):\n        word_counts = Counter(self.words)\n        return sorted(word_counts, key=word_counts.get, reverse=True)\n\n    def __len__(self):\n        return len(self.words_indexes) - self.sequence_length\n\n    def __getitem__(self, index):\n        input_text = self.words[index:index+self.sequence_length]\n        x = torch.LongTensor(self.words_indexes[index:index+self.sequence_length])\n        x_embed = torch.stack([self.word_embedding[word] for word in input_text])\n        y = torch.LongTensor(self.words_indexes[index+1:index+self.sequence_length+1])\n        return (x, x_embed, y)","metadata":{"id":"2XGVZdDR6pym","executionInfo":{"status":"ok","timestamp":1654511221111,"user_tz":-420,"elapsed":2374,"user":{"displayName":"Thang Phan","userId":"13927789615765580187"}},"execution":{"iopub.status.busy":"2022-07-21T13:45:00.894947Z","iopub.execute_input":"2022-07-21T13:45:00.896304Z","iopub.status.idle":"2022-07-21T13:45:00.914312Z","shell.execute_reply.started":"2022-07-21T13:45:00.896262Z","shell.execute_reply":"2022-07-21T13:45:00.912927Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_dataset = SherlockDataset('../input/sherlock-holmes/train_Sherlock.txt', embedding, 10)","metadata":{"id":"z-scvehc-3hA","executionInfo":{"status":"ok","timestamp":1654511222295,"user_tz":-420,"elapsed":1199,"user":{"displayName":"Thang Phan","userId":"13927789615765580187"}},"execution":{"iopub.status.busy":"2022-07-21T13:45:00.916266Z","iopub.execute_input":"2022-07-21T13:45:00.916807Z","iopub.status.idle":"2022-07-21T13:45:03.070211Z","shell.execute_reply.started":"2022-07-21T13:45:00.916764Z","shell.execute_reply":"2022-07-21T13:45:03.069267Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"len(train_dataset.uniq_words)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T13:45:03.071556Z","iopub.execute_input":"2022-07-21T13:45:03.072014Z","iopub.status.idle":"2022-07-21T13:45:03.078479Z","shell.execute_reply.started":"2022-07-21T13:45:03.071970Z","shell.execute_reply":"2022-07-21T13:45:03.077528Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"train_dataset.__getitem__(0)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T13:45:03.079836Z","iopub.execute_input":"2022-07-21T13:45:03.080946Z","iopub.status.idle":"2022-07-21T13:45:03.099291Z","shell.execute_reply.started":"2022-07-21T13:45:03.080894Z","shell.execute_reply":"2022-07-21T13:45:03.098451Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from torch import nn\n\nclass Model(nn.Module):\n    def __init__(self, dataset):\n        super(Model, self).__init__()\n        self.lstm_size = 128\n        self.embedding_dim = 300\n        self.num_layers = 3\n        self.dataset = dataset\n\n        vocab_size = len(dataset.uniq_words)\n        self.lstm = nn.LSTM(\n            input_size=self.embedding_dim,\n            hidden_size=self.lstm_size,\n            num_layers=self.num_layers,\n            dropout=0.1,\n        )\n        self.fc = nn.Linear(self.lstm_size, vocab_size)\n\n    def forward(self, embed, prev_state):\n        output, state = self.lstm(embed, prev_state)\n        logits = self.fc(output)\n        return logits, state\n\n    def init_state(self, sequence_length):\n        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n                torch.zeros(self.num_layers, sequence_length, self.lstm_size))","metadata":{"execution":{"iopub.status.busy":"2022-07-21T13:45:03.102373Z","iopub.execute_input":"2022-07-21T13:45:03.102764Z","iopub.status.idle":"2022-07-21T13:45:03.112762Z","shell.execute_reply.started":"2022-07-21T13:45:03.102732Z","shell.execute_reply":"2022-07-21T13:45:03.111660Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"model = Model(train_dataset).to(device)\n# model.load_state_dict(torch.load('../input/lstm-sherlock-state/lstm_state.pth'))\nmodel.load_state_dict(torch.load('../input/lstmsherlockstate2/lstm_state.pth', map_location='cpu'))\n# model.load_state_dict(torch.load('../input/lstm-sherlock-torchtext-500-epochs/lstm_state.pth'))","metadata":{"id":"Far2k3GjDS7W","executionInfo":{"status":"ok","timestamp":1654511222297,"user_tz":-420,"elapsed":12,"user":{"displayName":"Thang Phan","userId":"13927789615765580187"}},"execution":{"iopub.status.busy":"2022-07-21T13:45:03.114571Z","iopub.execute_input":"2022-07-21T13:45:03.115343Z","iopub.status.idle":"2022-07-21T13:45:03.280426Z","shell.execute_reply.started":"2022-07-21T13:45:03.115294Z","shell.execute_reply":"2022-07-21T13:45:03.279698Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch import nn, optim\nfrom torch.utils.data import DataLoader\n\ndef train(dataset, model, batch_size, max_epochs, sequence_length):\n    model.train()\n\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n#     optimizer.load_state_dict(torch.load('../input/lstm-sherlock-torchtext-500-epochs/lstm_optim.pth'))\n\n    for epoch in range(max_epochs):\n        state_h, state_c = model.init_state(sequence_length)\n        state_h = state_h.to(device)\n        state_c = state_c.to(device)\n\n        for batch, (x, x_embed, y) in enumerate(dataloader):\n            optimizer.zero_grad()\n\n            x, x_embed, y = x.to(device), x_embed.to(device), y.to(device)\n            # print(x_embed.shape)\n            y_pred, (state_h, state_c) = model(x_embed, (state_h, state_c))\n            # print(y_pred.shape)\n            loss = criterion(y_pred.transpose(1, 2), y)\n\n            state_h = state_h.detach()\n            state_c = state_c.detach()\n            \n            loss.backward()\n            optimizer.step()\n            if (batch + 1) % 50 == 0:\n                print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n        \n        torch.save(model.state_dict(), '/kaggle/working/lstm_state.pth')\n        torch.save(optimizer.state_dict(), '/kaggle/working/lstm_optim.pth')","metadata":{"id":"0HK9R2FCCu6r","executionInfo":{"status":"ok","timestamp":1654511226524,"user_tz":-420,"elapsed":349,"user":{"displayName":"Thang Phan","userId":"13927789615765580187"}},"execution":{"iopub.status.busy":"2022-07-21T13:45:03.281607Z","iopub.execute_input":"2022-07-21T13:45:03.282455Z","iopub.status.idle":"2022-07-21T13:45:03.294478Z","shell.execute_reply.started":"2022-07-21T13:45:03.282395Z","shell.execute_reply":"2022-07-21T13:45:03.293412Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# train(dataset=train_dataset,\n#       model=model,\n#       batch_size=256,\n#       max_epochs=500,\n#       sequence_length=10)","metadata":{"id":"RUyWt_d3DQw3","executionInfo":{"status":"error","timestamp":1654486279240,"user_tz":-420,"elapsed":44062,"user":{"displayName":"Thang Phan","userId":"13927789615765580187"}},"outputId":"2f6e4e43-e1cb-43df-ff97-ea292d71a6e0","execution":{"iopub.status.busy":"2022-07-21T13:45:03.296096Z","iopub.execute_input":"2022-07-21T13:45:03.296667Z","iopub.status.idle":"2022-07-21T13:45:03.311889Z","shell.execute_reply.started":"2022-07-21T13:45:03.296605Z","shell.execute_reply":"2022-07-21T13:45:03.310875Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# torch.save(model, \"/kaggle/working/lstm.pth\")","metadata":{"id":"wMyRABTSnxa7","execution":{"iopub.status.busy":"2022-07-21T13:45:03.313308Z","iopub.execute_input":"2022-07-21T13:45:03.313706Z","iopub.status.idle":"2022-07-21T13:45:03.322618Z","shell.execute_reply.started":"2022-07-21T13:45:03.313661Z","shell.execute_reply":"2022-07-21T13:45:03.321656Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Decoding","metadata":{}},{"cell_type":"code","source":"def custom_tokenize(prompt):\n    tokenized_sent = word_tokenize(prompt.lower())\n    # Remove punctuation\n    remove_punct = [word for word in tokenized_sent if re.search(r'\\w+', word) is not None]\n    return remove_punct","metadata":{"execution":{"iopub.status.busy":"2022-07-21T13:45:03.324384Z","iopub.execute_input":"2022-07-21T13:45:03.325359Z","iopub.status.idle":"2022-07-21T13:45:03.334376Z","shell.execute_reply.started":"2022-07-21T13:45:03.325308Z","shell.execute_reply":"2022-07-21T13:45:03.333639Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Search","metadata":{}},{"cell_type":"code","source":"def greedy_search_predict(dataset, embedding, model, prompt, next_words=20):    \n    model.eval()\n    words = custom_tokenize(prompt)\n    \n    state_h, state_c = model.init_state(len(words))\n    for i in range(0, next_words):\n        # x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n        x = torch.stack([embedding[word] for word in words[i:]])\n        x = x.reshape((1, len(words[i:]), embedding.dim))\n        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n\n        last_word_logits = y_pred[0][-1]\n        word_index = int(torch.argmax(last_word_logits))\n        words.append(dataset.index_to_word[word_index])\n    return words","metadata":{"execution":{"iopub.status.busy":"2022-07-21T15:04:51.499314Z","iopub.execute_input":"2022-07-21T15:04:51.499731Z","iopub.status.idle":"2022-07-21T15:04:51.508541Z","shell.execute_reply.started":"2022-07-21T15:04:51.499698Z","shell.execute_reply":"2022-07-21T15:04:51.507419Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"result = greedy_search_predict(train_dataset, embedding, model, 'logic')\nprint(result)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T15:05:04.653186Z","iopub.execute_input":"2022-07-21T15:05:04.653597Z","iopub.status.idle":"2022-07-21T15:05:04.675607Z","shell.execute_reply.started":"2022-07-21T15:05:04.653565Z","shell.execute_reply":"2022-07-21T15:05:04.674769Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"def beam_search_predict(dataset, embedding, model, prompt, beam_width, next_words=20):\n    model.eval()\n    words = custom_tokenize(prompt)\n    state_h, state_c = model.init_state(len(words))\n#     sequences = [{'sentence': [dataset.word_to_index[w] for w in words],\n#                   'sentence_embedding': [embedding[word] for word in words],\n#                   'score': 0, \n#                   'hidden_state': (state_h, state_c)}] # sequences store all generated results\n    sequences = [{'sentence': words,\n                  'sentence_embedding': [embedding[word] for word in words],\n                  'score': 0, \n                  'hidden_state': (state_h, state_c)}]\n    for i in range(next_words):\n        all_candidates = []\n        for sequence in sequences:\n            x = torch.stack(sequence['sentence_embedding'][i:])\n            x = x.reshape((1, len(words), 300))\n            y_pred, (state_h, state_c) = model(x, sequence['hidden_state'])\n\n            last_word_logits = y_pred[0][-1]\n            word_prob = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n            for word_index, prob in enumerate(word_prob):\n                candidate = {'sentence': sequence['sentence'] + [dataset.index_to_word[word_index]], 'score': sequence['score'] + prob}\n                candidate['sentence_embedding'] = sequence['sentence_embedding'] + [embedding[dataset.index_to_word[word_index]]]\n                candidate['hidden_state'] = (state_h, state_c)\n                all_candidates.append(candidate)\n        # Get top k candidate        \n        ordered = sorted(all_candidates, key=lambda x:x['score'], reverse=True)\n        sequences = ordered[:beam_width]\n\n\n    sentence_list = [sequence['sentence'] for sequence in sequences]\n    return sentence_list","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:25:48.467260Z","iopub.execute_input":"2022-07-21T14:25:48.467678Z","iopub.status.idle":"2022-07-21T14:25:48.480067Z","shell.execute_reply.started":"2022-07-21T14:25:48.467643Z","shell.execute_reply":"2022-07-21T14:25:48.479359Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"model = model.to('cpu')\nresult = beam_search_predict(train_dataset, embedding, model, 'logic', 5)\nfor sentence in result:\n    print(' '.join(sentence))","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:26:33.192323Z","iopub.execute_input":"2022-07-21T14:26:33.193001Z","iopub.status.idle":"2022-07-21T14:26:45.601580Z","shell.execute_reply.started":"2022-07-21T14:26:33.192943Z","shell.execute_reply":"2022-07-21T14:26:45.600514Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"## Sample","metadata":{}},{"cell_type":"code","source":"def random_sampling(logits, temperature):\n    logits = logits / temperature\n    word_prob = torch.nn.functional.softmax(logits, dim=0).detach().numpy()\n    word_index = np.random.choice(len(word_prob), p=word_prob)\n    return word_index\n\ndef topk_sampling(logits, k):\n    word_prob = torch.nn.functional.softmax(logits, dim=0).detach().numpy()\n    # Get index of k largest prob\n    k_largest_index = np.argpartition(word_prob, -k)[-k:]\n    mask = np.zeros(word_prob.shape)\n    # Create mask to keep only k largest prob\n    mask[k_largest_index] = 1\n    word_prob = word_prob * mask\n    # Rescale so that sum = 1\n    word_prob = word_prob / word_prob.sum()\n    # Sample from new distribution\n    word_index = np.random.choice(len(word_prob), p=word_prob)\n    return word_index\n\ndef nucleus_sampling(logits, p):\n    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n    word_prob = torch.nn.functional.softmax(sorted_logits, dim=0).detach().numpy()\n    cumulative_probs = np.cumsum(word_prob, axis=0)\n\n    # Remove tokens with cumulative probability above the threshold\n    sorted_indices_to_remove = cumulative_probs > p\n    # Shift the indices to the right to keep also the first token above the threshold\n    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].copy()\n    sorted_indices_to_remove[..., 0] = 0\n    # Set prob at index to be removed to 0 \n    indices_to_remove = sorted_indices[sorted_indices_to_remove]\n    word_prob[indices_to_remove] = 0\n    # Rescale so that sum = 1\n    word_prob = word_prob / word_prob.sum()\n    # Sample from new distribution\n    word_index = np.random.choice(len(word_prob), p=word_prob)\n    return word_index\n\ndef sample_predict(dataset, embedding, model, prompt, mode, temperature=1, k=5, p=0.9, next_words=20):\n    assert mode in ['random', 'top-k', 'nucleus']\n    \n    model.eval()\n    words = custom_tokenize(prompt)\n    \n    state_h, state_c = model.init_state(len(words))\n    for i in range(0, next_words):\n        # x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n        x = torch.stack([embedding[word] for word in words[i:]])\n        x = x.reshape((1, len(words[i:]), embedding.dim))\n        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n\n        last_word_logits = y_pred[0][-1]\n        if mode == 'random':\n            word_index = random_sampling(last_word_logits, temperature)\n        elif mode == 'top-k':\n            word_index = topk_sampling(last_word_logits, k)\n        elif mode == 'nucleus':\n            word_index = nucleus_sampling(last_word_logits, p)\n        words.append(dataset.index_to_word[word_index])\n\n    return words\n\n# print(' '.join(sample_predict(train_dataset, embedding, model, 'I could', 'random', temperature=0.9)))\n# print(' '.join(sample_predict(train_dataset, embedding, model, 'I could', 'top-k', k=10)))\n# print(' '.join(sample_predict(train_dataset, embedding, model, 'I could', 'nucleus', p=0.7)))","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:47:44.024066Z","iopub.execute_input":"2022-07-21T14:47:44.024509Z","iopub.status.idle":"2022-07-21T14:47:44.044294Z","shell.execute_reply.started":"2022-07-21T14:47:44.024464Z","shell.execute_reply":"2022-07-21T14:47:44.043525Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"print(' '.join(sample_predict(train_dataset, embedding, model, 'I could', 'random', temperature=0.9)))\nprint(' '.join(sample_predict(train_dataset, embedding, model, 'I could', 'top-k', k=40)))\nprint(' '.join(sample_predict(train_dataset, embedding, model, 'I could', 'nucleus', p=0.9)))","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:47:46.859922Z","iopub.execute_input":"2022-07-21T14:47:46.860333Z","iopub.status.idle":"2022-07-21T14:47:46.995884Z","shell.execute_reply.started":"2022-07-21T14:47:46.860298Z","shell.execute_reply":"2022-07-21T14:47:46.994871Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"# Experiment","metadata":{}},{"cell_type":"code","source":"class PromptDataset(torch.utils.data.Dataset):\n    def __init__(self, text_path, prompt_length):\n        self.prompts = self.load_sentences(text_path, prompt_length)\n        \n    def load_sentences(self, text_path, prompt_length=2):\n        prompts = []\n        with open(text_path, 'r') as f:\n            text = f.read()\n        # Split text into sentences\n        sentences = sent_tokenize(text.strip())\n        for sent in sentences[1:201]:\n            tokenized_sent = word_tokenize(sent.lower())\n            remove_punct = [word for word in tokenized_sent if re.search(r'\\w+', word) is not None]\n            prompt = ' '.join(remove_punct[:prompt_length])\n            real = ' '.join(remove_punct)\n            prompts.append((prompt, real))\n        return prompts\n    \n    def __len__(self):\n        return len(self.prompts)\n    \n    def __getitem__(self, idx):\n        return self.prompts[idx]","metadata":{"execution":{"iopub.status.busy":"2022-07-21T13:45:16.413236Z","iopub.execute_input":"2022-07-21T13:45:16.414039Z","iopub.status.idle":"2022-07-21T13:45:16.425224Z","shell.execute_reply.started":"2022-07-21T13:45:16.413995Z","shell.execute_reply":"2022-07-21T13:45:16.424115Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"test_data = PromptDataset('../input/sherlock-holmes/test_Sherlock.txt', 2)\ntest_loader = DataLoader(test_data, batch_size=1, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T13:45:16.426846Z","iopub.execute_input":"2022-07-21T13:45:16.427471Z","iopub.status.idle":"2022-07-21T13:45:16.540045Z","shell.execute_reply.started":"2022-07-21T13:45:16.427420Z","shell.execute_reply":"2022-07-21T13:45:16.539017Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\ndef count_repetition_percent(generated_strings):\n    words = generated_strings.split()\n    counter = Counter(words)\n    count_repeated = 0\n    for word in counter:\n        count_repeated += (counter[word] - 1)\n    sentence_length = sum(counter.values())\n    return count_repeated / sentence_length\n    ","metadata":{"execution":{"iopub.status.busy":"2022-07-21T14:02:32.209030Z","iopub.execute_input":"2022-07-21T14:02:32.209423Z","iopub.status.idle":"2022-07-21T14:02:32.215520Z","shell.execute_reply.started":"2022-07-21T14:02:32.209390Z","shell.execute_reply":"2022-07-21T14:02:32.214804Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def count_repetition_percent_modified(generated_strings):\n    words = generated_strings.split()\n    counter = Counter(words)\n    count_repeated = 0\n    for word in counter:\n        if word != '</s>':\n            count_repeated += (counter[word] - 1)\n    sentence_length = sum(counter.values()) - counter['</s>']\n    return count_repeated / sentence_length","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:50:53.575808Z","iopub.execute_input":"2022-07-21T16:50:53.576275Z","iopub.status.idle":"2022-07-21T16:50:53.583369Z","shell.execute_reply.started":"2022-07-21T16:50:53.576238Z","shell.execute_reply":"2022-07-21T16:50:53.582425Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"total_repeated_percent = 0\nfor idx, sample in enumerate(test_loader):\n    prompt = sample[0][0]\n    real = sample[1][0]\n    greedy_result = ' '.join(greedy_search_predict(train_dataset, embedding, model, prompt))\n    # repeated_percent = count_repetition_percent(greedy_result)\n    print(greedy_result)\n    repeated_percent = count_repetition_percent_modified(greedy_result)\n    total_repeated_percent += repeated_percent\navg_repeated_percent = total_repeated_percent / len(test_loader)\nprint(avg_repeated_percent)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T17:00:42.556069Z","iopub.execute_input":"2022-07-21T17:00:42.556501Z","iopub.status.idle":"2022-07-21T17:00:46.555354Z","shell.execute_reply.started":"2022-07-21T17:00:42.556468Z","shell.execute_reply":"2022-07-21T17:00:46.554623Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"total_repeated_percent = 0\nfor idx, sample in enumerate(test_loader):\n    prompt = sample[0][0]\n    real = sample[1][0]\n    beam_result = ' '.join(beam_search_predict(train_dataset, embedding, model, prompt, 5)[0])\n    print(beam_result)\n    repeated_percent = count_repetition_percent(beam_result)\n    total_repeated_percent += repeated_percent\navg_repeated_percent = total_repeated_percent / len(test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T15:09:38.637110Z","iopub.execute_input":"2022-07-21T15:09:38.637529Z","iopub.status.idle":"2022-07-21T15:52:32.921032Z","shell.execute_reply.started":"2022-07-21T15:09:38.637494Z","shell.execute_reply":"2022-07-21T15:52:32.920011Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"print(avg_repeated_percent)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:05:39.425312Z","iopub.execute_input":"2022-07-21T16:05:39.425825Z","iopub.status.idle":"2022-07-21T16:05:39.431531Z","shell.execute_reply.started":"2022-07-21T16:05:39.425785Z","shell.execute_reply":"2022-07-21T16:05:39.430514Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"code","source":"np.random.seed(0)\ntotal_repeated_percent = 0\nfor idx, sample in enumerate(test_loader):\n    prompt = sample[0][0]\n    real = sample[1][0]\n    random_result = ' '.join(sample_predict(train_dataset, embedding, model, prompt, 'random', temperature=1))\n    # repeated_percent = count_repetition_percent(random_result)\n    print(random_result)\n    repeated_percent = count_repetition_percent_modified(random_result)\n    total_repeated_percent += repeated_percent\navg_repeated_percent = total_repeated_percent / len(test_loader)\nprint(avg_repeated_percent)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T17:02:53.302761Z","iopub.execute_input":"2022-07-21T17:02:53.303185Z","iopub.status.idle":"2022-07-21T17:02:58.959570Z","shell.execute_reply.started":"2022-07-21T17:02:53.303150Z","shell.execute_reply":"2022-07-21T17:02:58.958387Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"np.random.seed(0)\ntotal_repeated_percent = 0\nfor idx, sample in enumerate(test_loader):\n    prompt = sample[0][0]\n    real = sample[1][0]\n    random_result2 = ' '.join(sample_predict(train_dataset, embedding, model, prompt, 'random', temperature=0.9))\n#     repeated_percent = count_repetition_percent(random_result2)\n    repeated_percent = count_repetition_percent_modified(random_result2)\n    total_repeated_percent += repeated_percent\navg_repeated_percent = total_repeated_percent / len(test_loader)\nprint(avg_repeated_percent)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:54:09.711566Z","iopub.execute_input":"2022-07-21T16:54:09.711975Z","iopub.status.idle":"2022-07-21T16:54:15.262136Z","shell.execute_reply.started":"2022-07-21T16:54:09.711942Z","shell.execute_reply":"2022-07-21T16:54:15.261441Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"np.random.seed(0)\ntotal_repeated_percent = 0\nfor idx, sample in enumerate(test_loader):\n    prompt = sample[0][0]\n    real = sample[1][0]\n    topk_result = ' '.join(sample_predict(train_dataset, embedding, model, prompt, 'top-k', k=40))\n#     repeated_percent = count_repetition_percent(topk_result)\n    print(topk_result)\n    repeated_percent = count_repetition_percent_modified(topk_result)\n    total_repeated_percent += repeated_percent\navg_repeated_percent = total_repeated_percent / len(test_loader)\nprint(avg_repeated_percent)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T17:12:08.120381Z","iopub.execute_input":"2022-07-21T17:12:08.120808Z","iopub.status.idle":"2022-07-21T17:12:15.066532Z","shell.execute_reply.started":"2022-07-21T17:12:08.120775Z","shell.execute_reply":"2022-07-21T17:12:15.065126Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"np.random.seed(0)\ntotal_repeated_percent = 0\nfor idx, sample in enumerate(test_loader):\n    prompt = sample[0][0]\n    real = sample[1][0]\n    topk_result = ' '.join(sample_predict(train_dataset, embedding, model, prompt, 'top-k', k=400))\n#     repeated_percent = count_repetition_percent(topk_result)\n    repeated_percent = count_repetition_percent_modified(topk_result)\n    total_repeated_percent += repeated_percent\navg_repeated_percent = total_repeated_percent / len(test_loader)\nprint(avg_repeated_percent)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T17:11:56.465194Z","iopub.execute_input":"2022-07-21T17:11:56.465635Z","iopub.status.idle":"2022-07-21T17:12:02.766666Z","shell.execute_reply.started":"2022-07-21T17:11:56.465600Z","shell.execute_reply":"2022-07-21T17:12:02.765689Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"np.random.seed(0)\ntotal_repeated_percent = 0\nfor idx, sample in enumerate(test_loader):\n    prompt = sample[0][0]\n    real = sample[1][0]\n    nucleus_result = ' '.join(sample_predict(train_dataset, embedding, model, prompt, 'nucleus', p=0.95))\n    print(nucleus_result)\n#     repeated_percent = count_repetition_percent(nucleus_result)\n    repeated_percent = count_repetition_percent_modified(nucleus_result)\n    total_repeated_percent += repeated_percent\navg_repeated_percent = total_repeated_percent / len(test_loader)\nprint(avg_repeated_percent)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:56:05.669604Z","iopub.execute_input":"2022-07-21T16:56:05.670081Z","iopub.status.idle":"2022-07-21T16:56:16.746193Z","shell.execute_reply.started":"2022-07-21T16:56:05.670042Z","shell.execute_reply":"2022-07-21T16:56:16.744994Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"np.random.seed(0)\ntotal_repeated_percent = 0\nfor idx, sample in enumerate(test_loader):\n    prompt = sample[0][0]\n    real = sample[1][0]\n    nucleus_result = ' '.join(sample_predict(train_dataset, embedding, model, prompt, 'nucleus', p=0.9))\n    print(nucleus_result)\n#     repeated_percent = count_repetition_percent(nucleus_result)\n    repeated_percent = count_repetition_percent_modified(nucleus_result)\n    total_repeated_percent += repeated_percent\navg_repeated_percent = total_repeated_percent / len(test_loader)\nprint(avg_repeated_percent)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:56:47.700656Z","iopub.execute_input":"2022-07-21T16:56:47.701192Z","iopub.status.idle":"2022-07-21T16:56:58.916636Z","shell.execute_reply.started":"2022-07-21T16:56:47.701147Z","shell.execute_reply":"2022-07-21T16:56:58.915452Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"total_repeated_percent = 0\nfor idx, sample in enumerate(test_loader):\n    prompt = sample[0][0]\n    real = sample[1][0]\n    \n    repeated_percent = count_repetition_percent(real)\n    total_repeated_percent += repeated_percent\navg_repeated_percent = total_repeated_percent / len(test_loader)\nprint(avg_repeated_percent)","metadata":{"execution":{"iopub.status.busy":"2022-07-21T16:08:24.483813Z","iopub.execute_input":"2022-07-21T16:08:24.484279Z","iopub.status.idle":"2022-07-21T16:08:24.503628Z","shell.execute_reply.started":"2022-07-21T16:08:24.484240Z","shell.execute_reply":"2022-07-21T16:08:24.502420Z"},"trusted":true},"execution_count":101,"outputs":[]}]}