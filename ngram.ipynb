{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlxLmKm0ymgs",
        "outputId": "11654c38-6298-4335-9ef6-10cd05f1c85e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import nltk\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import Laplace\n",
        "nltk.download('punkt')\n",
        "\n",
        "class PromptDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text_path, prompt_length):\n",
        "        self.prompts = self.load_sentences(text_path, prompt_length)\n",
        "        \n",
        "    def load_sentences(self, text_path, prompt_length=2):\n",
        "        prompts = []\n",
        "        with open(text_path, 'r') as f:\n",
        "            text = f.read()\n",
        "        # Split text into sentences\n",
        "        sentences = sent_tokenize(text.strip())\n",
        "        for sent in sentences[1:201]:\n",
        "            tokenized_sent = word_tokenize(sent.lower())\n",
        "            remove_punct = [word for word in tokenized_sent if re.search(r'\\w+', word) is not None]\n",
        "            prompt = ' '.join(remove_punct[:prompt_length])\n",
        "            real = ' '.join(remove_punct)\n",
        "            prompts.append((prompt, real))\n",
        "        return prompts\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.prompts)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.prompts[idx]\n",
        "\n",
        "\n",
        "def count_repetition_percent(generated_strings):\n",
        "    words = generated_strings.split()\n",
        "    counter = Counter(words)\n",
        "    count_repeated = 0\n",
        "    for word in counter:\n",
        "        count_repeated += (counter[word] - 1)\n",
        "    sentence_length = sum(counter.values())\n",
        "    return count_repeated / sentence_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NqhRB5qymgw"
      },
      "outputs": [],
      "source": [
        "with open(\"train_Sherlock.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "# text = text.replace(\"\\n\", \" \").replace(u'\\ufeff', '')\n",
        "text = re.sub(r\"ADVENTURE.{0,100}\\n\", \"\", text)\n",
        "text = re.sub(r'[^\\w\\s.]', '', text)\n",
        "text = re.sub(r\"\\n+\", \" \", text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYskNVexymgy"
      },
      "outputs": [],
      "source": [
        "tokenized_text = [list(map(str.lower, word_tokenize(sent))) \n",
        "                  for sent in sent_tokenize(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoeqfY8mymg1"
      },
      "outputs": [],
      "source": [
        "train_data, padded_sents = padded_everygram_pipeline(3, tokenized_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtXBQZa_ymg2"
      },
      "outputs": [],
      "source": [
        "model = Laplace(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Qkh8MHZymg3",
        "outputId": "8606ff24-72da-4f9f-a260-053b58058219"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(model.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAxo6xpHymg4",
        "outputId": "99ef655d-c09e-467b-c823-05af94e4df47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Vocabulary with cutoff=1 unk_label='<UNK>' and 7993 items>\n"
          ]
        }
      ],
      "source": [
        "model.fit(train_data, padded_sents)\n",
        "print(model.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDP5n7lHymg7"
      },
      "outputs": [],
      "source": [
        "count_dict = {}\n",
        "for word in model.vocab.__iter__():\n",
        "    count_dict[word] = model.counts[word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGUkR87jymg8",
        "outputId": "0ff2f719-83e1-4d7d-9abe-ef8dd52aa659"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['<s>',\n",
              " '</s>',\n",
              " '.',\n",
              " 'the',\n",
              " 'and',\n",
              " 'i',\n",
              " 'of',\n",
              " 'to',\n",
              " 'a',\n",
              " 'in',\n",
              " 'that',\n",
              " 'it',\n",
              " 'he',\n",
              " 'you',\n",
              " 'was',\n",
              " 'his',\n",
              " 'is',\n",
              " 'my',\n",
              " 'have',\n",
              " 'as',\n",
              " 'with',\n",
              " 'had',\n",
              " 'which',\n",
              " 'at',\n",
              " 'for',\n",
              " 'not',\n",
              " 'but',\n",
              " 'me',\n",
              " 'be',\n",
              " 'we',\n",
              " 'from',\n",
              " 'this',\n",
              " 'said',\n",
              " 'there',\n",
              " 'upon',\n",
              " 'holmes',\n",
              " 'him',\n",
              " 'so',\n",
              " 'she',\n",
              " 'her',\n",
              " 'very',\n",
              " 'been',\n",
              " 'your',\n",
              " 'what',\n",
              " 'no',\n",
              " 'all',\n",
              " 'on',\n",
              " 'one',\n",
              " 'then',\n",
              " 'were']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from heapq import nlargest\n",
        "\n",
        "nlargest(50, count_dict, key=count_dict.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiUKmfjQymg_"
      },
      "outputs": [],
      "source": [
        "from heapq import nlargest\n",
        "from random import choices\n",
        "\n",
        "\n",
        "def beam_search(model, vocab, current_sentences_score, ngram_order, beam_size=1):\n",
        "    candidate_sentences_with_score = {}\n",
        "\n",
        "    if len(current_sentences_score) == 0:\n",
        "        for word in vocab:\n",
        "            candidate_sentences_with_score[word] = model.logscore(word)\n",
        "\n",
        "    for sent in current_sentences_score.keys():\n",
        "        if sent.split()[-1] == \"</s>\":\n",
        "            candidate_sentences_with_score[sent] = current_sentences_score[sent]\n",
        "            continue\n",
        "\n",
        "        cur_length = len(sent.split())\n",
        "        if cur_length < ngram_order:\n",
        "            for word in vocab:\n",
        "                candidate_sentences_with_score[f\"{sent} {word}\"] = current_sentences_score[sent] + model.logscore(\n",
        "                    word, sent.split())\n",
        "        else:\n",
        "            for word in vocab:\n",
        "                candidate_sentences_with_score[f\"{sent} {word}\"] = current_sentences_score[sent] + model.logscore(\n",
        "                    word, sent.split()[-ngram_order+1:])\n",
        "\n",
        "    best_sentences = nlargest(\n",
        "        beam_size, candidate_sentences_with_score, key=candidate_sentences_with_score.get)\n",
        "    return {sent: candidate_sentences_with_score[sent] for sent in best_sentences}\n",
        "\n",
        "\n",
        "def sampling(model, vocab, current_sentence, ngram_order, top_k=None, top_p=None):\n",
        "    vocab_score = {}\n",
        "    if len(current_sentence) == 0:\n",
        "        for word in vocab:\n",
        "            vocab_score[word] = model.score(word)\n",
        "\n",
        "    else:\n",
        "        sent_split = current_sentence.split()\n",
        "        if sent_split[-1] == \"</s>\":\n",
        "            return current_sentence\n",
        "\n",
        "        if len(sent_split) < ngram_order:\n",
        "            for word in vocab:\n",
        "                vocab_score[word] = model.score(word, sent_split)\n",
        "        else:\n",
        "            for word in vocab:\n",
        "                vocab_score[word] = model.score(\n",
        "                    word, sent_split[-ngram_order+1:])\n",
        "\n",
        "    if top_k:\n",
        "        top_k_score = {}\n",
        "        top_k_words = nlargest(top_k, vocab_score, vocab_score.get)\n",
        "        for word in top_k_words:\n",
        "            top_k_score[word] = vocab_score[word]\n",
        "        current_sentence += \" {}\".format(\n",
        "            choices(list(top_k_score.keys()), weights=list(top_k_score.values()))[0])\n",
        "\n",
        "    elif top_p:\n",
        "        sorted_word_score = sorted(\n",
        "            vocab_score.items(), key=lambda x: x[1], reverse=True)\n",
        "        top_p_score = {}\n",
        "        curr_p = 0\n",
        "        for word, score in sorted_word_score:\n",
        "            if curr_p > top_p:\n",
        "                break\n",
        "            top_p_score[word] = score\n",
        "            curr_p += score\n",
        "\n",
        "        current_sentence += \" {}\".format(\n",
        "            choices(list(top_p_score.keys()), weights=list(top_p_score.values()))[0])\n",
        "\n",
        "    else:\n",
        "        current_sentence += \" {}\".format(\n",
        "            choices(\n",
        "                list(vocab_score.keys()),\n",
        "                weights=list(vocab_score.values())\n",
        "            )[0]\n",
        "        )\n",
        "    return current_sentence\n",
        "\n",
        "\n",
        "def generate_sent(model, mode, num_words=20, init_sent=\"<s>\", beam_size=3, top_k=None, top_p=None):\n",
        "    assert mode in [\"greedy\", \"beam\", \"sampling\", \"nucleus\"]\n",
        "    vocab = list(model.vocab.__iter__())\n",
        "    vocab.remove(\"<s>\")\n",
        "\n",
        "    if mode == \"greedy\":\n",
        "        current_sentences_score = {init_sent: 0}\n",
        "        for i in range(num_words):\n",
        "            current_sentences_score = beam_search(\n",
        "                model, vocab, current_sentences_score, model.order, beam_size=1)\n",
        "        return current_sentences_score\n",
        "\n",
        "    if mode == \"beam\":\n",
        "        current_sentences_score = {init_sent: 0}\n",
        "        for i in range(num_words):\n",
        "            current_sentences_score = beam_search(\n",
        "                model, vocab, current_sentences_score, model.order, beam_size=beam_size)\n",
        "        return current_sentences_score\n",
        "\n",
        "    if mode == \"sampling\":\n",
        "        current_sentence = init_sent\n",
        "        for i in range(num_words):\n",
        "            current_sentence = sampling(\n",
        "                model, vocab, current_sentence, model.order, top_k, top_p)\n",
        "        return current_sentence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01xmfq2QymhA",
        "outputId": "77002bdf-7709-4a43-af57-c71cccade9cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy: \n",
            "{'<s> what do you think that i have been a very serious one to the other side . </s>': -38.550964165354756}\n",
            "\n",
            "Beam: \n",
            "{'<s> what could it be i could see that you will excuse my saying so something just a baby her wee hand': -27.823510507283483, '<s> what could it be i could see that you will excuse my saying so something just a little more . </s>': -30.87033136403648, '<s> what could it be i could see that you will excuse my saying so something just a little too much .': -32.38490453686624}\n",
            "\n",
            "Random sampling: \n",
            "<s> what a blind fool i have spoiled him . </s>\n",
            "\n",
            "Top k sampling: \n",
            "<s> what did you observe anything very funny cried our client has rather an early appointment this morning had hurried up the\n",
            "\n",
            "Nucleus sampling: \n",
            "<s> what can it mean i gasped . </s>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "init_sent = \"<s> what\"\n",
        "\n",
        "print(\"Greedy: \\n{}\\n\".format(generate_sent(model, \"beam\", init_sent=init_sent, beam_size=1)))\n",
        "print(\"Beam: \\n{}\\n\".format(generate_sent(model, \"beam\", init_sent=init_sent, beam_size=3)))\n",
        "print(\"Random sampling: \\n{}\\n\".format(generate_sent(model, \"sampling\", init_sent=init_sent)))\n",
        "print(\"Top k sampling: \\n{}\\n\".format(generate_sent(model, \"sampling\", init_sent=init_sent, top_k=100)))\n",
        "print(\"Nucleus sampling: \\n{}\\n\".format(generate_sent(model, \"sampling\", init_sent=init_sent, top_p=0.9)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4Kdt3PmymhA"
      },
      "outputs": [],
      "source": [
        "test_data = PromptDataset('./test_Sherlock.txt', 2)\n",
        "test_loader = DataLoader(test_data, batch_size=1, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUudH2HaymhB",
        "outputId": "d9a8eea1-ea0d-44ac-a53e-f4369bd39ce4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n"
          ]
        }
      ],
      "source": [
        "generated_dict = {\n",
        "    'greedy': [],\n",
        "    'beam': [],\n",
        "    'random': [],\n",
        "    'top_k': [],\n",
        "    'top_p_95': [],\n",
        "    'top_p_90': []\n",
        "}\n",
        "\n",
        "for i, prompt in enumerate(test_loader):\n",
        "    context = prompt[0][0]\n",
        "\n",
        "    greedy = list(generate_sent(model, \"beam\", init_sent=context, beam_size=1).keys())[0]\n",
        "    generated_dict['greedy'].append(greedy)\n",
        "\n",
        "    beam = list(generate_sent(model, \"beam\", init_sent=context, beam_size=5).keys())[0]\n",
        "    generated_dict['beam'].append(beam)\n",
        "\n",
        "    random = generate_sent(model, \"sampling\", init_sent=context)\n",
        "    generated_dict['random'].append(random)\n",
        "\n",
        "    top_k = generate_sent(model, \"sampling\", init_sent=context, top_k=40)\n",
        "    generated_dict['top_k'].append(top_k)\n",
        "\n",
        "    top_p_95 = generate_sent(model, \"sampling\", init_sent=context, top_p=0.95)\n",
        "    generated_dict['top_p_95'].append(top_p_95)\n",
        "\n",
        "    top_p_90 = generate_sent(model, \"sampling\", init_sent=context, top_p=0.90)\n",
        "    generated_dict['top_p_90'].append(top_p_90)\n",
        "\n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlNO1Mq-ymhC"
      },
      "outputs": [],
      "source": [
        "rep_percent = {\n",
        "    'greedy': None,\n",
        "    'beam': None,\n",
        "    'random': None,\n",
        "    'top_k': None,\n",
        "    'top_p_95': None,\n",
        "    'top_p_90': None\n",
        "}\n",
        "\n",
        "for type in generated_dict.keys():\n",
        "    sum_percent = 0\n",
        "    for sent in generated_dict[type]:\n",
        "        sent = re.sub(r'</s>', '', sent)\n",
        "        sent = re.sub(r'[.]', '', sent)\n",
        "        sum_percent += count_repetition_percent(sent)\n",
        "    rep_percent[type] = sum_percent / len(generated_dict[type])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV4Rq67VymhD",
        "outputId": "6bd2c904-e424-4d75-826f-e5ec4bd3362b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'greedy': 0.28457900147238424, 'beam': 0.2691777597402601, 'random': 0.2940398232873082, 'top_k': 0.19146011992393588, 'top_p_95': 0.2974515826021249, 'top_p_90': 0.3018054517721705}\n"
          ]
        }
      ],
      "source": [
        "print(rep_percent)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ngram.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('dl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "46cf0ceb771b6d3daf409ddfc7a8baa7b210722c7f57f87657e5fad79b22a2c0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
